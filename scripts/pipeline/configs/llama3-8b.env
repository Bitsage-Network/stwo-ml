# Obelysk Model Preset: Llama 3.1 8B
# ═══════════════════════════════════════════════════════════
MODEL_NAME="llama3-8b"
MODEL_HF="meta-llama/Llama-3.1-8B"
MODEL_LAYERS=32
MODEL_QUANT="symmetric8"
MODEL_SIZE_GB=16
MODEL_SHARDS=4
MODEL_HIDDEN=4096
MODEL_INTERMEDIATE=14336
MODEL_NUM_ATTENTION_HEADS=32
MODEL_NUM_KEY_VALUE_HEADS=8
MODEL_ACTIVATION="swiglu"
MODEL_VOCAB_SIZE=128256
MODEL_MAX_POSITION=131072
DESCRIPTION="Llama 3.1 8B — 32 transformer layers, GQA (32 heads / 8 KV heads), SwiGLU activation"
