# Obelysk Model Preset: Llama 3.1 70B
# ═══════════════════════════════════════════════════════════
MODEL_NAME="llama3-70b"
MODEL_HF="meta-llama/Llama-3.1-70B"
MODEL_LAYERS=80
MODEL_QUANT="symmetric8"
MODEL_SIZE_GB=140
MODEL_SHARDS=16
MODEL_HIDDEN=8192
MODEL_INTERMEDIATE=28672
MODEL_NUM_ATTENTION_HEADS=64
MODEL_NUM_KEY_VALUE_HEADS=8
MODEL_ACTIVATION="swiglu"
MODEL_VOCAB_SIZE=128256
MODEL_MAX_POSITION=131072
DESCRIPTION="Llama 3.1 70B — 80 transformer layers, GQA (64 heads / 8 KV heads), SwiGLU activation. Requires multi-GPU (140GB+ VRAM)."
